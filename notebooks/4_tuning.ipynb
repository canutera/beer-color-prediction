{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosen models\n",
    "- DecisionTree with 2 features, 5 splits and outliers\n",
    "- GradientBoostingRegressor with no outliers, 3 splits and 9 features\n",
    "- Lasso with 5 features, 2 splits and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "> Note: Best features are in order according to importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 rows contain at least one outlier\n",
      "Outlier ratio: 20.67%\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../data/5_selected_Kbest/selected_Kbest.csv').sort_values('job_id').drop('job_id', axis=1)\n",
    "\n",
    "def outliers_index(df, threshold=3):\n",
    "    z_scores = np.abs(stats.zscore(df))\n",
    "    threshold = 3\n",
    "    outliers = df[z_scores > threshold]\n",
    "    \n",
    "    print(f'{len(outliers[outliers.notnull().any(axis=1)])} rows contain at least one outlier')\n",
    "    print('Outlier ratio:', f'{len(outliers[outliers.notnull().any(axis=1)])/len(df):.2%}')\n",
    "    return outliers.notnull().any(axis=1)\n",
    "outliers = outliers_index(df)\n",
    "\n",
    "df_no_out = df[~outliers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hold some rows for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set len with outliers: 120\n",
      "train len without outliers: 96\n",
      "\n",
      "test set len with outliers: 24\n",
      "test len without outliers: 19\n"
     ]
    }
   ],
   "source": [
    "# train set\n",
    "df = df.iloc[:-int(len(df)*0.2), :]\n",
    "df_no_out = df_no_out.iloc[:-int(len(df_no_out)*0.2), :]\n",
    "print('train set len with outliers:', len(df))\n",
    "print('train len without outliers:', len(df_no_out))\n",
    "\n",
    "# test set\n",
    "test = df.iloc[-int(len(df)*0.2):, :]\n",
    "test_no_out = df_no_out.iloc[-int(len(df_no_out)*0.2):, :]\n",
    "print('\\ntest set len with outliers:', len(test))\n",
    "print('test len without outliers:', len(test_no_out))\n",
    "\n",
    "test.to_csv('../data/7_model_specific_data_sets/test_with_outliers.csv', index=False)\n",
    "test_no_out.to_csv('../data/7_model_specific_data_sets/test_without_outliers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_state = 42\n",
    "\n",
    "lgbm_parameters={\"reg_alpha\": [0.005, 0.03, 0.07, 0.1, 0.2,],\n",
    "                 \"reg_lambda\": [0.005, 0.03, 0.07, 0.1, 0.2,],\n",
    "                 \"learning_rate\" : [1e-3,1e-2, 1e-1],\n",
    "                 \"n_estimators\": [ 100, 200, 500],\n",
    "                 \"num_leaves\": [8, 16, 32],\n",
    "                 \"max_depth\": [2,3,-1],\n",
    "                \n",
    "                  }\n",
    "\n",
    "lgbm_tss = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "lgbm_scaler = StandardScaler()\n",
    "\n",
    "\n",
    "lgbm_scaled = lgbm_scaler.fit_transform(df)\n",
    "\n",
    "lgbm_X = lgbm_scaled[:,:15]\n",
    "lgbm_y = lgbm_scaled[:, -1:]\n",
    "\n",
    "# lgbm_tuning_model = GridSearchCV(estimator=LGBMRegressor(random_state=random_state),\n",
    "#                                  param_grid=lgbm_parameters,\n",
    "#                                  scoring='neg_mean_squared_error',\n",
    "#                                  cv=lgbm_tss, verbose=3, n_jobs=-1, error_score='raise')\n",
    "\n",
    "# lgbm_tuning_model.fit(lgbm_X, lgbm_y)\n",
    "# lgbm_tuning_model.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "2 features, 5 splits and outliers\n",
    "> best params: {'max_depth': 1, 'max_leaf_nodes': None, 'min_samples_leaf': 5, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.1, 'splitter': 'best'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "dt_parameters={\"splitter\": [\"best\",\"random\"],\n",
    "               \"max_depth\": [1,3,5,7,9,11,12],\n",
    "               \"min_samples_leaf\": [5,7,10,15,20],\n",
    "               \"min_samples_split\": [5,10,15,20,30,40,50],\n",
    "               \"min_weight_fraction_leaf\": [0.1,0.2,0.3,0.5],\n",
    "               \"max_leaf_nodes\": [None,10,20,30,40]}\n",
    "\n",
    "dt_tss = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "dt_scaler = StandardScaler()\n",
    "\n",
    "dt_X = dt_scaler.fit_transform(df.iloc[:, :2])\n",
    "dt_scaled = dt_scaler.fit_transform(df)\n",
    "\n",
    "dt_X = dt_scaled[:,:2]\n",
    "dt_y = dt_scaled[:,-1]\n",
    "\n",
    "# dt_tuning_model = GridSearchCV(estimator=DecisionTreeRegressor(random_state=random_state),\n",
    "#                                param_grid=dt_parameters,\n",
    "#                                scoring='neg_mean_squared_error',\n",
    "#                                cv=dt_tss, verbose=1, n_jobs=-1)\n",
    "\n",
    "# dt_tuning_model.fit(dt_X, dt_y)\n",
    "# dt_tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "5 features, 2 splits and outliers\n",
    "\n",
    "> best params: {'alpha': 0.07, 'max_iter': 100, 'tol': 0.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_parameters={\"alpha\": [0.005, 0.02, 0.03, 0.05, 0.06, 0.07, 0.1, 0.2, 0.3, 0.5, 0.6, 0.7],\n",
    "                  \"max_iter\": [100, 200, 500, 1000],\n",
    "                  \"tol\": [1e-2, 5e-2, 1e-3, 5e-3, 1e-4]\n",
    "                  }\n",
    "\n",
    "lasso_tss = TimeSeriesSplit(n_splits=2)\n",
    "\n",
    "lasso_scaler = StandardScaler()\n",
    "lasso_scaled = lasso_scaler.fit_transform(df)\n",
    "\n",
    "lasso_X = lasso_scaled[:,:5]\n",
    "lasso_y = lasso_scaled[:,-1]\n",
    "\n",
    "# lasso_tuning_model = GridSearchCV(estimator=Lasso(random_state=random_state),\n",
    "#                                   param_grid=lasso_parameters,\n",
    "#                                   scoring='neg_mean_squared_error',\n",
    "#                                   cv=lasso_tss, verbose=1,  n_jobs=-1)\n",
    "# lasso_tuning_model.fit(lasso_X, lasso_y)\n",
    "# lasso_tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostedTree\n",
    "no outliers, 3 splits and 9 features - Training 2h \n",
    "\n",
    ">{'alpha': 0.005,\n",
    "> 'ccp_alpha': 0.025,\n",
    "> 'learning_rate': 0.1,\n",
    "> 'max_leaf_nodes': None,\n",
    "> 'min_samples_leaf': 10,\n",
    "> 'min_samples_split': 30,\n",
    "> 'min_weight_fraction_leaf': 0.1,\n",
    "> 'n_estimators': 500,\n",
    "> 'tol': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gbt_parameters={\"alpha\": [0.005, 0.03, 0.07, 0.1,],\n",
    "                \"learning_rate\" : [1e-3,1e-2, 1e-1],\n",
    "                \"n_estimators\": [ 100, 200],\n",
    "                \"min_samples_leaf\": [10, 30],\n",
    "                \n",
    "                \"min_weight_fraction_leaf\": [0.1,0.2,0.5,],\n",
    "                \"tol\": [1e-2, 1e-3, 1e-4],\n",
    "                \"max_leaf_nodes\": [None,4,8,16],\n",
    "                \"ccp_alpha\": [2e-3,1e-2,25e-3],\n",
    "                  }\n",
    "\n",
    "gbt_tss = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "gbt_scaler = StandardScaler()\n",
    "gbt_scaled = gbt_scaler.fit_transform(df_no_out)\n",
    "\n",
    "gbt_X = gbt_scaled[:,:9]\n",
    "gbt_y = gbt_scaled[:, -1]\n",
    "\n",
    "# gbt_tuning_model = GridSearchCV(estimator=GradientBoostingRegressor(random_state=random_state),\n",
    "#                                 param_grid=gbt_parameters,\n",
    "#                                 scoring='neg_mean_squared_error',\n",
    "#                                 cv=gbt_tss, verbose=1, n_jobs=-1)\n",
    "\n",
    "# gbt_tuning_model.fit(gbt_X, gbt_y)\n",
    "# gbt_tuning_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "l = {'alpha': 0.2, 'max_iter': 100, 'tol': 0.01}\n",
    "d = {'max_depth': 1,\n",
    "     'max_leaf_nodes': None,\n",
    "     'min_samples_leaf': 5,\n",
    "     'min_samples_split': 50,\n",
    "     'min_weight_fraction_leaf': 0.5,\n",
    "     'splitter': 'best'}\n",
    "g = {'alpha': 0.005,\n",
    "     'ccp_alpha': 0.025,\n",
    "     'learning_rate': 0.1,\n",
    "     'max_leaf_nodes': None,\n",
    "     'min_samples_leaf': 10,\n",
    "     'min_samples_split': 30,\n",
    "     'min_weight_fraction_leaf': 0.1,\n",
    "     'n_estimators': 500,\n",
    "     'tol': 0.01}\n",
    "gbt = GradientBoostingRegressor(**g, random_state=random_state).fit(gbt_X, gbt_y)\n",
    "lasso = Lasso(**l, random_state=random_state).fit(lasso_X, lasso_y)\n",
    "dt = DecisionTreeRegressor(**d, random_state=random_state).fit(dt_X, dt_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = {'learning_rate': 0.1,\n",
    " 'max_depth': 3,\n",
    " 'n_estimators': 500,\n",
    " 'num_leaves': 16,\n",
    " 'reg_alpha': 0.2,\n",
    " 'reg_lambda': 0.005}\n",
    "lgbm = LGBMRegressor(**lg, random_state=random_state).fit(lgbm_X, lgbm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def train_with_n_features(estimator, n_features, df):\n",
    "    TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    X = scaled[:,:n_features]\n",
    "    y = scaled[:, -1]\n",
    "    estimator.fit(X, y)\n",
    "    if estimator.__class__.__name__ == \"LGBMRegressor\":\n",
    "        estimator.booster_.save_model(f'../models/n_features_test/{estimator.__class__.__name__}_{n_features}.text')\n",
    "    else:\n",
    "        with open(f'../models/n_features_test/{estimator.__class__.__name__}_{n_features}.pkl','wb') as file:\n",
    "            pickle.dump(estimator, file)\n",
    "    \n",
    "    # save scalers\n",
    "    with open(f'../models/scalers/{estimator.__class__.__name__}_{n_features}_scaler.pkl','wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "\n",
    "for i in [2,4,5,8,10,12,15]:\n",
    "    for model in [gbt, lasso, dt, lgbm]:\n",
    "        train_with_n_features(model, i, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save models and datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "_m = {'gbt':[gbt, gbt_scaler], 'lasso':[lasso, lasso_scaler], 'dt':[dt, dt_scaler, dt_scaler], 'lgbm':[lgbm, lgbm_scaler]}\n",
    "\n",
    "for i in _m:\n",
    "    #save models\n",
    "    with open(f'../models/{i}.pkl','wb') as file:\n",
    "        pickle.dump(_m[i][0], file)\n",
    "    \n",
    "    # save scalers\n",
    "    with open(f'../models/scalers/{i}_scaler.pkl','wb') as file:\n",
    "        pickle.dump(_m[i][1], file)\n",
    "lgbm.booster_.save_model('../models/lgbm.txt')\n",
    "\n",
    "# datasets\n",
    "lgbm_scaled = pd.DataFrame(lgbm_scaled, columns=df.columns)[list(df.columns[:9]) + ['color']]\n",
    "lgbm_scaled.to_csv('../data/7_model_specific_data_sets/gbt_scaled.csv', index=False)\n",
    "\n",
    "gbt_scaled = pd.DataFrame(gbt_scaled, columns=df.columns)[list(df.columns[:9]) + ['color']]\n",
    "gbt_scaled.to_csv('../data/7_model_specific_data_sets/gbt_scaled.csv', index=False)\n",
    "\n",
    "lasso_scaled = pd.DataFrame(lasso_scaled, columns=df.columns)[list(df.columns[:5]) + ['color']]\n",
    "lasso_scaled.to_csv('../data/7_model_specific_data_sets/lasso_scaled.csv', index=False)\n",
    "\n",
    "dt_scaled = pd.DataFrame(dt_scaled, columns=df.columns)[list(df.columns[:2]) + ['color']]\n",
    "dt_scaled.to_csv('../data/7_model_specific_data_sets/dt_scaled.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
