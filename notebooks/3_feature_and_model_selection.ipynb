{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('../data/4_engineered/engineered_features.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Literal\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error as mae, mean_squared_error as mse\n",
    "def baseline_dummy_metrics(df:pd.DataFrame, samples:int, strategy:Literal['median','mean']):\n",
    "    '''estimate baseline values for dataset using sklearn dummy regressor\n",
    "    samples is the number of fits to be done\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "    df with r2_score, mean absolute error, mean squared error and std deviations respectively\n",
    "    average for every fit \n",
    "    '''\n",
    "    values = pd.DataFrame()\n",
    "    # scale features using zscore\n",
    "    X = df.drop('color', axis=1)\n",
    "    X = stats.zscore(X)\n",
    "    y = df['color']\n",
    "    y = stats.zscore(y)\n",
    "    for i in range(samples):\n",
    "        \n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=random.randint(1,1000))\n",
    "\n",
    "        dummy_regr = DummyRegressor(strategy=strategy)\n",
    "        dummy_regr.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = dummy_regr.predict(X_test)\n",
    "        values = pd.concat([values, pd.DataFrame.from_records({'r2_score':[r2_score(y_test,y_pred)], 'mae':[mae(y_test,y_pred)], 'mse':[mse(y_test,y_pred)]})], ignore_index=True)\n",
    "    _dict = {}\n",
    "    for col in values.columns:\n",
    "        _dict[col] = values[col].mean()\n",
    "        _dict[f'{col}_std'] = values[col].std()\n",
    "    return pd.DataFrame.from_records([_dict])\n",
    "\n",
    "baseline = baseline_dummy_metrics(df, 10000, 'median')\n",
    "print('Baseline values for metrics')\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def outliers_index(df, threshold=3):\n",
    "#apply the z-score method and get abs \n",
    "    z_scores = np.abs(stats.zscore(df))\n",
    "    threshold = 3\n",
    "    outliers = df[z_scores > threshold]\n",
    "    \n",
    "    print(f'{len(outliers[outliers.notnull().any(axis=1)])} rows contain at least one outlier')\n",
    "    print('Outlier ratio:', f'{len(outliers[outliers.notnull().any(axis=1)])/len(df):.2%}')\n",
    "    return outliers.notnull().any(axis=1)\n",
    "outliers = outliers_index(df)\n",
    "df_no_out = df[~outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "df = df.iloc[:-int(len(df)*0.2), :]\n",
    "df_no_out = df_no_out.iloc[:-int(len(df_no_out)*0.2), :]\n",
    "print('train set len with outliers:', len(df))\n",
    "print('train len without outliers:', len(df_no_out))\n",
    "\n",
    "# test set\n",
    "test = df.iloc[-int(len(df)*0.2):, :]\n",
    "test_no_out = df_no_out.iloc[-int(len(df_no_out)*0.2):, :]\n",
    "print('\\ntest set len with outliers:', len(test))\n",
    "print('test len without outliers:', len(test_no_out))\n",
    "\n",
    "test.to_csv('../data/7_model_specific_data_sets/test_with_outliers.csv', index=False)\n",
    "test_no_out.to_csv('../data/7_model_specific_data_sets/test_without_outliers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Kbest features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "def select_k_best(df, name=None):\n",
    "    X = df.drop(['job_id','color'], axis=1)\n",
    "    y = df.color\n",
    "    selector = SelectKBest(f_regression, k=len(X.columns))\n",
    "    # selector.set_output(transform='pandas')\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    selected = pd.concat([X_new, y.to_frame('color')], axis=1)\n",
    "    \n",
    "    #save keeping job_id to order in time properly\n",
    "    df[['job_id']].join(selected).to_csv(f'../data/5_selected_Kbest/selected_Kbest_{name}.csv', index=False)\n",
    "    return selected\n",
    "\n",
    "Kbest = select_k_best(df, '')\n",
    "Kbest_no_out = select_k_best(df, 'no_out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "random_state = 42\n",
    "models = []\n",
    "\n",
    "models.append(LinearRegression())\n",
    "models.append(Pipeline(steps=[('polyFeature',PolynomialFeatures() ),('regressor', LinearRegression())]))\n",
    "models.append(Ridge(random_state=random_state))\n",
    "models.append(Lasso(random_state=random_state))\n",
    "models.append(ElasticNet(random_state=random_state))\n",
    "models.append(SVR())\n",
    "models.append(GradientBoostingRegressor(random_state=random_state))\n",
    "models.append(DecisionTreeRegressor(random_state=random_state))\n",
    "models.append(RandomForestRegressor(random_state=random_state))\n",
    "models.append(LGBMRegressor(random_state=random_state))\n",
    "\n",
    "\n",
    "scorers = dict()\n",
    "scorers['r2_score'] = make_scorer(r2_score, greater_is_better=True)\n",
    "scorers['mae'] = make_scorer(mae, greater_is_better=False)\n",
    "scorers['mse'] = make_scorer(r2_score, greater_is_better=False)\n",
    "\n",
    "def search(estimator, range_time_split, range_features,scorers, outliers,  X, y):\n",
    "    df = pd.DataFrame()\n",
    "    # print(estimator.__class__.__name__)\n",
    "    estimator_1 = clone(estimator)\n",
    "    estimator_2 = clone(estimator)\n",
    "    \n",
    "    # with outliers\n",
    "    X_out = StandardScaler().fit_transform(X)\n",
    "    y_out = StandardScaler().fit_transform(y).flatten()\n",
    "    \n",
    "\n",
    "    # without outliers\n",
    "    X_ = StandardScaler().fit_transform(X[~outliers])\n",
    "    y_ = StandardScaler().fit_transform(y[~outliers]).flatten()\n",
    "    \n",
    "    \n",
    "    for n_splits in range_time_split:\n",
    "        tss = TimeSeriesSplit(n_splits = n_splits)\n",
    "\n",
    "        for n_features in range_features:\n",
    "            for scorer in scorers:\n",
    "                \n",
    "                # with outliers\n",
    "                scores = cross_val_score(estimator_1, X_out[:, :n_features], y_out, cv=tss, scoring=scorers[scorer])\n",
    "                _dict = {'model':[estimator.__class__.__name__], 'n_features':[n_features], 'n_splits':[n_splits], 'scorer':[scorer],'outliers':'yes', 'scores':[scores]}\n",
    "                new =  pd.DataFrame(_dict)\n",
    "                df = pd.concat([df, new], ignore_index=True)\n",
    "                \n",
    "                # without outliers\n",
    "                scores = cross_val_score(estimator_2, X_[:, :n_features], y_, cv=tss, scoring=scorers[scorer])\n",
    "                _dict = {'model':[estimator.__class__.__name__], 'n_features':[n_features], 'n_splits':[n_splits], 'scorer':[scorer], 'outliers':'no', 'scores':[scores]}\n",
    "                new =  pd.DataFrame(_dict)\n",
    "                df = pd.concat([df, new], ignore_index=True)\n",
    "    return df\n",
    "outliers = outliers_index(df)\n",
    "models = pd.concat([search(m, range(2,6), range(2, 16), scorers, outliers,  X_new,y.to_frame()) for m in tqdm(models)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv('../data/5_model_selection/models.csv', index=False)\n",
    "baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.assign(mean_scores=a.scores.apply(np.mean)).reset_index(drop=True)\n",
    "b = b.pivot(index=['model','n_features', 'outliers', 'n_splits',], columns='scorer', values=['mean_scores']).reset_index().sort_values(('mean_scores', 'r2_score')).reset_index(drop=True)\n",
    "b.columns = ['model', 'n_features', 'outliers', 'n_splits', 'mae', 'mse','r2_score']\n",
    "b = b.assign(**{i:b[i].abs() for i in ['mae', 'mse', ]})\n",
    "b.to_csv('../data/5_model_selection/pivoted_models.csv')\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model by mae\n",
    "\n",
    "DecisionTree with 2 features, 5 splits and outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.sort_values('mae').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model by r2_score \n",
    "GradientBoostingRegressor with no outliers, 3 splits and 9 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.sort_values('r2_score', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model by mse\n",
    "Since GradientBoostingRegressor with no outliers, 3 splits and 9 features was also the best in this metric, Lasso with 5 features, 2 splits and outliers will be chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.sort_values('mse',).head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
